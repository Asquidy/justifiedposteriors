<!doctype html>
<html lang="en">
  <head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <link rel="icon" type="image/png" href="/assets/img/logo.png" />
  <link rel="apple-touch-icon" href="/assets/img/logo.png" />
  <link rel="stylesheet" href="/assets/css/style.css" />
  <link rel="alternate" type="application/rss+xml" title="Justified Posteriors" href="/feed.xml" />
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Home | Justified Posteriors</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Home" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Justified Posteriors is a podcast by economists Andrey Fradkin and Seth Benzell featuring conversations about about the economics of AI and innovation." />
<meta property="og:description" content="Justified Posteriors is a podcast by economists Andrey Fradkin and Seth Benzell featuring conversations about about the economics of AI and innovation." />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="Justified Posteriors" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Home" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"Justified Posteriors is a podcast by economists Andrey Fradkin and Seth Benzell featuring conversations about about the economics of AI and innovation.","headline":"Home","name":"Justified Posteriors","url":"http://localhost:4000/"}</script>
<!-- End Jekyll SEO tag -->

</head>

  <body>
    <main class="site-main" id="main">
      <div class="wrap">
        <section class="hero">
  <img src="/assets/img/logo.png" alt="Justified Posteriors" class="hero__logo">
  <h1 class="hero__title">Justified Posteriors</h1>
  <p class="hero__subtitle">A podcast exploring the economics of AI and innovation by deeply engaging with the literature.</p>
  <p class="hero__authors">By Andrey Fradkin & Seth Benzell</p>

  <div class="platforms">
    <a href="https://podcasts.apple.com/us/podcast/justified-posteriors/id1784022930" class="platform-link platform-link--apple" target="_blank" rel="noopener">
      <svg viewBox="0 0 24 24" fill="currentColor">
        <path d="M12.152 6.896c-.948 0-2.415-1.078-3.96-1.04-2.04.027-3.91 1.183-4.961 3.014-2.117 3.675-.546 9.103 1.519 12.09 1.013 1.454 2.208 3.09 3.792 3.039 1.52-.065 2.09-.987 3.935-.987 1.831 0 2.35.987 3.96.948 1.637-.026 2.676-1.48 3.676-2.948 1.156-1.688 1.636-3.325 1.662-3.415-.039-.013-3.182-1.221-3.22-4.857-.026-3.04 2.48-4.494 2.597-4.559-1.429-2.09-3.623-2.324-4.39-2.376-2-.156-3.675 1.09-4.61 1.09zM15.53 3.83c.843-1.012 1.4-2.427 1.245-3.83-1.207.052-2.662.805-3.532 1.818-.78.896-1.454 2.338-1.273 3.714 1.338.104 2.715-.688 3.559-1.701"/>
      </svg>
      Apple Podcasts
    </a>

    <a href="https://open.spotify.com/show/6GAE7zHAnQyTaOprfEqJtQ" class="platform-link platform-link--spotify" target="_blank" rel="noopener">
      <svg viewBox="0 0 24 24" fill="currentColor">
        <path d="M12 0C5.4 0 0 5.4 0 12s5.4 12 12 12 12-5.4 12-12S18.66 0 12 0zm5.521 17.34c-.24.359-.66.48-1.021.24-2.82-1.74-6.36-2.101-10.561-1.141-.418.122-.779-.179-.899-.539-.12-.421.18-.78.54-.9 4.56-1.021 8.52-.6 11.64 1.32.42.18.479.659.301 1.02zm1.44-3.3c-.301.42-.841.6-1.262.3-3.239-1.98-8.159-2.58-11.939-1.38-.479.12-1.02-.12-1.14-.6-.12-.48.12-1.021.6-1.141C9.6 9.9 15 10.561 18.72 12.84c.361.181.54.78.241 1.2zm.12-3.36C15.24 8.4 8.82 8.16 5.16 9.301c-.6.179-1.2-.181-1.38-.721-.18-.601.18-1.2.72-1.381 4.26-1.26 11.28-1.02 15.721 1.621.539.3.719 1.02.419 1.56-.299.421-1.02.599-1.559.3z"/>
      </svg>
      Spotify
    </a>

    <a href="https://www.youtube.com/@justifiedposteriors" class="platform-link platform-link--youtube" target="_blank" rel="noopener">
      <svg viewBox="0 0 24 24" fill="currentColor">
        <path d="M23.498 6.186a3.016 3.016 0 0 0-2.122-2.136C19.505 3.545 12 3.545 12 3.545s-7.505 0-9.377.505A3.017 3.017 0 0 0 .502 6.186C0 8.07 0 12 0 12s0 3.93.502 5.814a3.016 3.016 0 0 0 2.122 2.136c1.871.505 9.376.505 9.376.505s7.505 0 9.377-.505a3.015 3.015 0 0 0 2.122-2.136C24 15.93 24 12 24 12s0-3.93-.502-5.814zM9.545 15.568V8.432L15.818 12l-6.273 3.568z"/>
      </svg>
      YouTube
    </a>

    <a href="https://empiricrafting.substack.com/feed" class="platform-link platform-link--rss" target="_blank" rel="noopener">
      <svg viewBox="0 0 24 24" fill="currentColor">
        <path d="M6.503 20.752c0 1.794-1.456 3.248-3.251 3.248-1.796 0-3.252-1.454-3.252-3.248 0-1.794 1.456-3.248 3.252-3.248 1.795.001 3.251 1.454 3.251 3.248zm-6.503-12.572v4.811c6.05.062 10.96 4.966 11.022 11.009h4.817c-.062-8.71-7.118-15.758-15.839-15.82zm0-3.368c10.58.046 19.152 8.594 19.183 19.188h4.817c-.03-13.231-10.755-23.954-24-24v4.812z"/>
      </svg>
      RSS
    </a>
  </div>

  <div class="secondary-links">
    <a href="https://empiricrafting.substack.com/" class="secondary-link" target="_blank" rel="noopener">
      <svg viewBox="0 0 24 24" fill="currentColor">
        <path d="M22.539 8.242H1.46V5.406h21.08v2.836zM1.46 10.812V24L12 18.11 22.54 24V10.812H1.46zM22.54 0H1.46v2.836h21.08V0z"/>
      </svg>
      Read on Substack
    </a>
  </div>
</section>


<section class="beliefs-section">
  <div class="beliefs-section__header">
    <h2 class="beliefs-section__main-title">Belief Updates Tracker</h2>
    <p class="beliefs-section__subtitle">How we updated our priors across 22 episodes</p>
  </div>

  <div class="beliefs-filter">
    <div class="search-container">
      <svg class="search-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <circle cx="11" cy="11" r="8"></circle>
        <line x1="21" y1="21" x2="16.65" y2="16.65"></line>
      </svg>
      <input
        type="text"
        id="beliefs-search"
        class="beliefs-search"
        placeholder="Search episodes, papers, or beliefs..."
      >
    </div>
  </div>

  <div class="beliefs-table-container" id="beliefs-table">
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">Techno-Prophets Try Macroeconomics</span>
        <span class="belief-row__paper">Epoch AI's GATE model projecting 23% GDP growth in 2027</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">The probability of 23% GDP growth in 2027 is approximately 1 in 1000. While the model is technically impressive, I'm deeply skeptical that such dramatic growth is achievable in the near term.</td>
                <td class="belief-content">Also approximately 1 in 1000 probability for 23% growth in 2027. The baseline seems extraordinarily optimistic.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">I moved a tiny bit on a tiny bit - essentially unchanged. The model is useful for bridging the conversation between technologists and economists, but it hasn't convinced me that explosive growth is imminent.</td>
                <td class="belief-content">Unchanged, possibly moved slightly away from belief. The model needs explicit policy levers and more realistic saving dynamics before I'd update meaningfully.</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">The GATE model represents a valuable attempt to formalize how AI might affect macroeconomic growth, but both hosts found the 23% figure implausible. The model's main contribution may be in providing a common framework for technologists and economists to discuss AI's economic potential, rather than generating reliable forecasts. The lack of realistic policy mechanisms and savings behavior limits its predictive value.</p>
        </div>
        
      </div>
    </article>
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">Evaluating GDPVal</span>
        <span class="belief-row__paper">OpenAI's economic value task evaluation measuring AI perf...</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">AI win rate against expert humans on substantive tasks is around 10%. In two years, I expect 90% of knowledge workers will still be producing artifacts 'by hand' rather than managing AI agents.</td>
                <td class="belief-content">AI win rate vs experts around 10%. Only about 5% of workers will be primarily managing AI agents by 2027, with 95% still working collaboratively or independently.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">Updated AI win rate from 10% to 30% - a significant shift. Still expect 85% of workers to be producing by hand in two years. The key insight is that 'taste' and subjective preferences matter enormously - there's only 70% inter-human agreement on quality.</td>
                <td class="belief-content">Updated win rate from 10% to 25%. The 95% collaborative figure remains. The most important takeaway is that model progress dramatically dwarfs prompt engineering improvements.</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">Both hosts significantly updated their beliefs about AI capability after seeing OpenAI's GDPVal results, roughly tripling their estimated win rate against experts. However, they remained skeptical about rapid workforce transformation. A crucial finding was the low inter-human agreement rate (70%), suggesting that much of what we evaluate as 'quality' reflects subjective taste rather than objective performance. This limits how much AI can definitively 'beat' humans when the target itself is fuzzy.</p>
        </div>
        
      </div>
    </article>
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">Are We There Yet?</span>
        <span class="belief-row__paper">METR's 'Measuring AI Ability to Complete Long Tasks' trac...</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">50/50 probability that AI can complete a human-month equivalent task within 5 years. Almost certain (very high confidence) within 10 years.</td>
                <td class="belief-content">50/50 in 5 years for human-month task completion. Greater than 90% probability within 10 years.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">More confident in the 5-year timeline based on personal experience with Opus 4.5. The 10-year estimate remains unchanged. However, the domain tested is too narrow - it's mostly software engineering, which may not generalize.</td>
                <td class="belief-content">5-year estimate unchanged but error bars are larger. 10-year estimate dropped from 90% to 70-80%. The paper was less impressive when I dug into the methodology and domain coverage.</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">The METR paper prompted divergent updates: Andrey became more confident in near-term AI capability based on direct experience, while Seth became more uncertain about the 10-year horizon after scrutinizing the methodology. Both noted that the narrow focus on software engineering tasks limits generalizability to broader cognitive work. The paper highlights the difficulty of creating meaningful benchmarks that capture economically relevant capabilities rather than artificial test scenarios.</p>
        </div>
        
      </div>
    </article>
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">One LLM to Rule Them All</span>
        <span class="belief-row__paper">Analysis of LLM competitive dynamics and multi-homing beh...</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">Multi-homing behavior among users suggests genuine horizontal differentiation between models - different models are better for different purposes, which would support a competitive market.</td>
                <td class="belief-content">Models are primarily vertically differentiated - some are simply better than others. Multi-homing exists mainly due to legacy systems and backwards compatibility needs, not because different models excel at different things.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">60% probability the industry becomes more concentrated over the next 2 years. The market is more competitive than search engines but will likely end up less concentrated than smartphone operating systems. Multi-homing may reflect legacy systems and backwards compatibility rather than true differentiation.</td>
                <td class="belief-content">85% probability of increased concentration in 2 years. Horizontal differentiation happens at the application layer, not at the foundation model level. The market dynamics favor consolidation.</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">Both hosts expect significant market consolidation in AI models, though Seth is more confident (85% vs 60%). The key disagreement resolved around whether multi-homing indicates healthy competition or transitional friction. They concluded that while users might switch between models today, this likely reflects immature infrastructure rather than genuine product differentiation. True horizontal differentiation may emerge at the application layer built on top of foundation models, not among the models themselves.</p>
        </div>
        
      </div>
    </article>
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">When Humans and Machines Don't Say</span>
        <span class="belief-row__paper">Braghieri et al. on preference falsification in surveys; ...</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">For preference falsification, I expected transgender athletes in sports would show the largest gap between private and public opinions (45%), with blackface Halloween costumes showing no gap. For AI chain-of-thought, I thought reasoning was pretty good but not perfect.</td>
                <td class="belief-content">Expected racial microaggressions to show the largest gap between public and private opinions. For AI transparency, I put 50% probability that systems can be built that never lie or hide their reasoning.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">Surprised that effects were uniformly small (0.1-0.2 standard deviations) across topics. Racial microaggressions actually showed the largest gap. On AI reasoning, I now believe systems are fundamentally limited in self-understanding.</td>
                <td class="belief-content">Confirmed that campus issues show larger gaps. Updated probability that AI transparency is fundamentally limited from 50% to 60-70%. The Anthropic research suggests chain-of-thought may not faithfully represent actual reasoning processes.</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">The preference falsification research revealed that social desirability bias operates fairly uniformly across controversial topics, rather than being dramatically higher for any single issue. Both hosts were surprised by this finding. The parallel discussion of AI chain-of-thought faithfulness raised deeper concerns: if AI systems cannot reliably explain their own reasoning, interpretability and alignment become harder problems. Seth substantially updated toward believing transparent AI may be fundamentally impossible, not just technically challenging.</p>
        </div>
        
      </div>
    </article>
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">Claude Just Refereed</span>
        <span class="belief-row__paper">Anthropic's Economic Index analyzing AI task usage patter...</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">Expected most AI usage to be for writing and programming tasks, reflecting the obvious strengths of language models.</td>
                <td class="belief-content">Estimated coding and writing would account for roughly 80% of usage.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">Confirmed on writing and programming dominance, but surprised by how low office and administrative task usage was despite clear capability. Usage reflects adoption barriers - legal fears, conservative management - not capability limitations.</td>
                <td class="belief-content">Surprised by the lack of managerial task usage like scheduling and time management. The data can't distinguish homework from professional use, which limits interpretation.</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">The Anthropic Economic Index provided unique ground-truth data on how people actually use AI, revealing a significant gap between capability and adoption. While AI demonstrably can help with office administration and scheduling, usage in these areas remains low. This suggests that organizational and cultural barriers - risk aversion, unclear liability, professional identity - are currently more binding constraints than technical limitations. The path to economic impact runs through adoption, not just capability development.</p>
        </div>
        
      </div>
    </article>
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">We Won't Be Missed</span>
        <span class="belief-row__paper">Restrepo's theoretical model of work and growth dynamics ...</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">Skeptical that asymptotic results about labor share approaching zero tell us much about the next 100 years. The long-run predictions are too speculative to be actionable.</td>
                <td class="belief-content">Greater than 90% probability of large decline in labor share. Less than 10% probability of labor share hitting approximately 0% within 100 years. 70% probability that real wages continue rising post-AGI.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">Unchanged. The paper is mathematically interesting but I remain unconvinced that limit theorems provide practical insight about medium-term dynamics.</td>
                <td class="belief-content">Unchanged on labor share trajectory. Real wage prediction moved marginally from 70% to 69% - essentially no update. The model didn't provide evidence that changed my views.</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">Despite engaging seriously with Restrepo's formal model of post-AGI labor dynamics, neither host updated their beliefs. The paper explores an important question - what happens to labor's share of income when machines can do everything - but both hosts felt the asymptotic framing was too disconnected from policy-relevant timeframes. The real question isn't whether labor share eventually approaches zero, but how the transition unfolds and whether real wages can continue rising during it.</p>
        </div>
        
      </div>
    </article>
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">Did Meta's Algorithms Swing the 2020 Election?</span>
        <span class="belief-row__paper">Guess et al. (Science) experimental study of algorithmic ...</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">80% confident that algorithmic effects on political attitudes would be very small. Social media effects are overstated.</td>
                <td class="belief-content">67% probability that algorithms put significant thumb on the scale of political attitudes and behavior.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">Reconsidering but remaining cautious about content moderation effects. The ML component of algorithms seems less important than previously thought. The study reinforced my skepticism but raised questions about what else might matter.</td>
                <td class="belief-content">Major update: dropped from 67% to 5% for polarization effects, 30% for candidate preference effects. This was my largest belief update. Realized impact is much lower than potential impact - even if algorithms could manipulate users, they apparently don't much.</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">This paper produced Seth's largest documented belief update, dropping his concern about algorithmic polarization from 67% to 5%. The experimental design - randomly switching users to chronological feeds - provided unusually clean causal evidence. The finding that sophisticated recommendation algorithms barely affect political attitudes suggests either that content matters more than ordering, or that users' predispositions dominate algorithmic influence. This substantially deflates concerns about social media companies deliberately polarizing users for engagement.</p>
        </div>
        
      </div>
    </article>
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">Scaling Laws Meet Persuasion</span>
        <span class="belief-row__paper">Research on diminishing returns for AI-generated politica...</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">Very high confidence in diminishing returns - better AI doesn't translate linearly to more persuasion. Less than 1% probability that super-persuasive AI represents an existential risk.</td>
                <td class="belief-content">99% confident in diminishing returns for AI persuasion. 60% probability that super-persuasive AI could be an existential-level concern.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">Increased to 99.9% confidence in diminishing returns. X-risk from super-persuasion remains below 1%. Persuasion is fundamentally a social process, not one-to-one communication.</td>
                <td class="belief-content">Increased to 99.9% on diminishing returns. Reduced X-risk concern from 60% to 55%. Current models aren't that persuasive yet, and there's no clear path to superintelligent persuaders.</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">Both hosts converged on near-certainty (99.9%) that more capable AI models face diminishing returns in persuasion. This finding has important implications for AI safety discussions: if persuasion naturally saturates, then concerns about superintelligent manipulators may be overblown. Seth notably reduced his X-risk estimate, though still maintains meaningful concern. The deeper insight is that persuasion operates through social networks and repeated exposure, not single compelling arguments, which limits how much any technology can supercharge it.</p>
        </div>
        
      </div>
    </article>
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">The Simple Macroeconomics of AI</span>
        <span class="belief-row__paper">Daron Acemoglu's task-based model predicting less than 1 ...</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">Effects will be small if AI adoption remains limited to 'using ChatGPT a little bit.' Could be huge if AI accelerates scientific research and discovery.</td>
                <td class="belief-content">Expected 2-4 percentage points additional GDP growth over 20-50 years from AI adoption.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">Moved down my median estimate for AI's growth impact. Diffusion limitations will constrain macro effects even if capability advances rapidly. Acemoglu's careful task-based accounting is persuasive.</td>
                <td class="belief-content">Cut growth estimate roughly in half. Now estimate around 30 basis points annually rather than closer to 100. It's hard to read something this carefully reasoned and not update significantly.</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">Acemoglu's rigorous task-based analysis had a sobering effect on both hosts' growth expectations. The model accounts for which tasks AI can actually automate, how quickly adoption spreads, and what fraction of economic activity those tasks represent. The conclusion that AI adds less than 1 percentage point to growth is far below silicon valley hype but methodologically sound. Capital accumulation and diffusion dynamics are the binding constraints, not AI capability per se.</p>
        </div>
        
      </div>
    </article>
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">Beyond Task Replacement</span>
        <span class="belief-row__paper">Timothy Bresnahan's analysis of General Purpose Technolog...</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">Roughly 20% of AI's economic value comes from direct task replacement. The remaining 80% comes from innovation, new products, robotics, and organizational transformation.</td>
                <td class="belief-content">Estimated 30-50% of AI's economic value would come from direct task replacement of human labor.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">Moved significantly toward Bresnahan's framing. Reorganization and capital deepening are more accurate ways to think about AI's economic impact than tallying up automated tasks.</td>
                <td class="belief-content">Reduced task replacement contribution to below previous estimates. The task replacement framing is actively misleading about how technological transformation works.</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">Bresnahan's GPT framework shifted both hosts toward thinking about AI as enabling organizational transformation rather than automating individual tasks. Historical parallels with electricity and computing suggest that the largest gains come from restructuring work processes to take advantage of new capabilities, not from one-for-one substitution. This has policy implications: focusing on which jobs AI will 'replace' misses the more important question of how organizations will reorganize around AI capabilities.</p>
        </div>
        
      </div>
    </article>
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">Is Social Media a Trap?</span>
        <span class="belief-row__paper">Bertoni et al. on TikTok and Instagram as 'collective tra...</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">The collective trap hypothesis is pretty reasonable. Some users might be worse off in equilibrium even if they're choosing freely.</td>
                <td class="belief-content">Very implausible that the average person is worse off from social media. Initially estimated 5-10% of users might be harmed.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">Surprised by the magnitude of effects in the data, but skeptical of the methodology. This may reflect cultural preference rather than genuine economic inefficiency.</td>
                <td class="belief-content">Doubled estimate of harmed users from 5-10% to 20%. Still convinced net effect is positive on average, but the harm is more widespread than I expected.</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">The collective trap framing suggests users might rationally choose social media while being made worse off - like everyone standing at a concert because everyone else is standing. Seth notably doubled his estimate of harmed users while maintaining that average effects are positive. The methodological concerns highlight how difficult it is to measure welfare effects of freely chosen activities. Whether this represents inefficiency requiring intervention or simply revealed preference for a new form of entertainment remains contested.</p>
        </div>
        
      </div>
    </article>
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">AI and Labor Market Effects</span>
        <span class="belief-row__paper">Ide and Talamas 'Artificial Intelligence and the Knowledg...</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">65% probability that only 2% of workers will be primarily managing AI agents within 5 years. 55% probability that AI exacerbates wage polarization.</td>
                <td class="belief-content">60% probability on the 2% management figure. 25% probability AI exacerbates polarization.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">Management adoption estimate unchanged. Polarization estimate unchanged at 55%. Without modeling aggregate demand, I can't take macro predictions from these frameworks seriously.</td>
                <td class="belief-content">Management estimate unchanged. Polarization probability increased from 25% to 30%. The top must benefit until AI can literally do everything - the question is whether the middle or bottom loses more.</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">The paper's framework distinguishing AI-as-worker, AI-as-manager, and AI-as-expert provides useful conceptual clarity, but both hosts remained skeptical of specific predictions. The polarization question - does AI help the middle class or hollow it out - saw modest movement, with Seth becoming slightly more concerned. Both emphasized that partial equilibrium models miss crucial feedback effects through wages, employment, and aggregate demand that determine actual distributional outcomes.</p>
        </div>
        
      </div>
    </article>
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">Can Political Science Contribute?</span>
        <span class="belief-row__paper">Henry Farrell's 'AI is Governance' from Annual Review of ...</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">60-70% probability that AI requires fundamentally new conceptual frameworks from political science. About 10% probability that AI is primarily a cultural technology.</td>
                <td class="belief-content">Ambiguous on new frameworks. 40% probability AI is primarily a cultural technology shaping how humans interact and communicate.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">Increased to 90% probability that political scientists haven't adequately conceptualized AI. It represents governance challenges they haven't thought through. Cultural technology framing unchanged.</td>
                <td class="belief-content">Updated cultural technology framing from 40% to 80-90% - AI is significantly about culture, not just capability. But as the primary way to understand AI, only increased from 10% to 15%.</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">Farrell's framing of AI as governance - systems that shape behavior through rules, incentives, and constraints - resonated strongly with both hosts. Andrey substantially updated toward believing political science lacks adequate frameworks for AI, while Seth embraced the cultural technology dimension. The insight that AI is as much about social coordination and norm-setting as about capability suggests policy discussions focused narrowly on safety or automation may miss larger questions about power and social organization.</p>
        </div>
        
      </div>
    </article>
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">Emergency Pod: AI Already Causing Labor Effects?</span>
        <span class="belief-row__paper">Brynjolfsson et al. 'Canaries in the Coal Mine' documenti...</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">About 50% probability we would see evidence of slower young worker employment growth in AI-exposed fields.</td>
                <td class="belief-content">About 70% probability of early evidence of AI labor effects.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">95% confidence the descriptive pattern is accurate, but uncertain on causal interpretation. The timing - late 2022 - doesn't match when AI should bite. Could be interest rates or other factors.</td>
                <td class="belief-content">75% probability that early-stage workers are particularly challenged by AI. Technology is often skill-biased in ways that hurt new entrants. This dims my vision for Generation Alpha's career prospects.</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">The paper documents a concerning pattern: employment growth for young workers in AI-exposed occupations has slowed relative to other workers. Both hosts found the descriptive evidence convincing but struggled with causal interpretation. The timing coincides with interest rate increases, making it hard to isolate AI effects. If confirmed as causal, this suggests AI may not replace workers directly but rather reduce demand for new entrants - a more subtle but still significant labor market transformation with implications for career development and training.</p>
        </div>
        
      </div>
    </article>
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">Should AI Read Without Permission?</span>
        <span class="belief-row__paper">'Books Encounters' analyzing how pirated Books3 training ...</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">Less than 1% improvement in model quality from including Books3 data. Marginal training data has diminishing returns at scale.</td>
                <td class="belief-content">Similar to Andrey - diminishing marginal returns from additional training data.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">Even more marginal than expected. The memorization rates were surprisingly low, suggesting models extract concepts rather than text.</td>
                <td class="belief-content">Wrong on initial prediction about measurable improvement. Moved toward 'wait and see' on copyright implications. If you want to read the book, you'd actually read it - models don't reproduce content meaningfully.</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">The paper addressed the heated copyright debate over AI training data by measuring actual impact of the Books3 dataset. The finding that this data contributes marginally to performance - and that memorization rates are surprisingly low - suggests the copyright debate may be less about economic harm to authors and more about principles. Models appear to extract abstract patterns rather than reproducible content, complicating arguments that training constitutes infringement.</p>
        </div>
        
      </div>
    </article>
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">How Much Should We Invest in AI Safety?</span>
        <span class="belief-row__paper">Chad Jones papers on optimal AI investment given existent...</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">Willing to tolerate less than 1% additional existential risk per year (specifically around 0.5%). Skeptical that GDP-scale safety investments (suggested 1.8-15.8%) are warranted.</td>
                <td class="belief-content">Willing to tolerate multiple percentage points of existential risk per year. 10% confidence that significant GDP fraction should go to safety.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">Moved risk tolerance up slightly from 0.5% to about 1% per year. Remain skeptical of massive safety investments but more willing to accept some risk for AI benefits.</td>
                <td class="belief-content">Not moved on risk tolerance. Skeptical that the AI safety community's claimed risks are well-calibrated. Concerned about Pascal's Mugger dynamics - tiny probability of huge harm justifying any expenditure.</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">The Jones papers force explicit tradeoffs between AI development speed and safety spending. Both hosts engaged seriously but remained skeptical of high safety investment recommendations. Seth articulated concern about Pascal's Mugger - that existential risk arguments can justify unlimited spending based on tiny probabilities of huge harms. The discussion highlighted how much depends on contested probability estimates that are difficult to validate empirically.</p>
        </div>
        
      </div>
    </article>
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">If Robots Are Coming, Why Aren't Interest Rates Higher?</span>
        <span class="belief-row__paper">Chow et al. using real interest rates to infer market exp...</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">Only 10% probability that low interest rates indicate markets don't believe in near-term AGI. Many alternative mechanisms could explain rates.</td>
                <td class="belief-content">90% probability that low rates indicate markets don't expect imminent AGI. If AGI were coming, investment demand would drive rates up.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">Moved slightly to 10-20% but remain skeptical of macro-finance inference about AI timelines. Wealth effects, demographic changes, and other factors complicate interpretation.</td>
                <td class="belief-content">Increased to 95%, convinced by the investment demand logic. If companies really expected transformative AI returns, they'd invest more aggressively, pushing rates up.</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">The paper uses an elegant revealed-preference argument: if AGI were imminent, the expected returns to capital investment would be enormous, driving up interest rates. The fact that rates remain low suggests markets don't believe AGI is near. Seth found this convincing while Andrey remained skeptical that macro aggregates can cleanly identify AI expectations. The timing ambiguity - how far in advance would rates move - makes this an imperfect test but an interesting data point against near-term AGI forecasts.</p>
        </div>
        
      </div>
    </article>
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">Can AI Make Better Decisions Than Doctors?</span>
        <span class="belief-row__paper">Mullainathan and Obermeyer 'Diagnosing Physician Error' u...</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">90% confident doctors make systematic errors that could be identified. 50% probability that ML methods could reliably flag these errors.</td>
                <td class="belief-content">About 80% confident in systematic physician errors. Moderate-high confidence ML could help identify them.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">Increased doctor error confidence to 95%. Dramatically increased ML identification probability from 50% to 85%, convinced by the natural experiment design.</td>
                <td class="belief-content">Error confidence increased to 85%. ML identification increased to 85%, matching Andrey. The natural experiment showing doctors miss high-risk patients who happened not to be tested was compelling.</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">Both hosts substantially updated on AI's ability to improve medical decisions after seeing the natural experiment evidence. The paper compared patients who were tested versus those who weren't, finding that untested patients often had higher risk profiles that ML could identify. This suggests significant efficiency gains from AI-assisted diagnosis. However, both emphasized that adoption remains the bottleneck - technical capability to improve decisions exists, but integrating AI into clinical workflows faces organizational, legal, and professional resistance.</p>
        </div>
        
      </div>
    </article>
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">Robots for the Retired</span>
        <span class="belief-row__paper">Acemoglu and Restrepo 'Demographics and Automation' linki...</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">40% probability the backward-looking evidence (aging explains 20% of variation in robot adoption) is economically significant. 70% probability that aging increases automation going forward.</td>
                <td class="belief-content">75% probability backward evidence is economically significant. 60% probability aging drives future automation.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">Backward estimate increased from 40% to 50%. Forward estimate increased from 70% to 75%. The natural experiment using demographic variation across countries was convincing.</td>
                <td class="belief-content">Backward increased from 75% to 85%. Forward increased from 60% to 65%. The key insight is that automation creates new products, not just job substitution.</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">The paper uses cross-country variation in aging to identify causal effects on robot adoption, finding that older workforces drive automation investment. Both hosts increased their confidence in this relationship. The finding that aging populations adopt more robots suggests automation may be partly a response to labor scarcity rather than purely technological opportunity. This has optimistic implications: automation might arrive precisely when needed to offset declining workforces rather than creating unemployment.</p>
        </div>
        
      </div>
    </article>
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">A Resource Curse for AI</span>
        <span class="belief-row__paper">Drago and Lane 'The Intelligence Curse' exploring whether...</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">50-55% probability elites become less responsive to citizen preferences by 2050 due to AI. 20% probability of a new social contract emerging to address this.</td>
                <td class="belief-content">60% probability of reduced elite responsiveness by 2050. 20% probability of new social contract.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">Increased elite unresponsiveness probability from 50% to 57%. New social contract probability unchanged at 20%. Property rights are critical to prevent totalitarian outcomes.</td>
                <td class="belief-content">Both unchanged at 60% and 20%. Many contingent steps required for either the dystopian or optimistic scenarios to unfold.</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">The 'intelligence curse' analogy to resource curses suggests that AI might allow elites to become less dependent on citizen cooperation, reducing incentives for responsive governance. Both hosts found this concerning but noted the many contingent steps required. The historical parallel is imperfect - oil wealth provides direct revenue while AI still requires human workers and consumers. Whether AI concentration enables authoritarian governance depends on technical architecture and institutional responses we can still influence.</p>
        </div>
        
      </div>
    </article>
    
    <article class="belief-row">
      <button class="belief-row__header" aria-expanded="false">
        <span class="belief-row__title">Situational Awareness</span>
        <span class="belief-row__paper">Leopold Aschenbrenner's essay on AI scaling trajectories,...</span>
        <span class="belief-row__toggle" aria-hidden="true">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <polyline points="6 9 12 15 18 9"></polyline>
          </svg>
        </span>
      </button>

      <div class="belief-row__content" hidden>
        <div class="belief-table-wrapper">
          <table class="belief-table">
            <thead>
              <tr>
                <th class="belief-table__col-label"></th>
                <th class="belief-table__col-host">Andrey</th>
                <th class="belief-table__col-host">Seth</th>
              </tr>
            </thead>
            <tbody>
              <tr class="belief-row--prior">
                <td class="belief-label">Prior</td>
                <td class="belief-content">AGI is way more than 5 years away. Skeptical that linear extrapolation of scaling laws guarantees AGI. Doubtful that superhuman AI solves geopolitical problems.</td>
                <td class="belief-content">Takes 5-10 year AGI timeline seriously but with reservations. Concerned but not panicked about alignment challenges.</td>
              </tr>
              <tr class="belief-row--posterior">
                <td class="belief-label">Posterior</td>
                <td class="belief-content">Maintaining skepticism on timelines. A 'world model of the internet' may be incomplete for AGI - real-world sensor data matters. Still doubt superintelligence solves all problems.</td>
                <td class="belief-content">Moved toward accepting the national security framing. If AGI is developed, it needs government control rather than remaining in private hands. 'Won't be able to identify when we're one breakthrough away.'</td>
              </tr>
            </tbody>
          </table>
        </div>

        
        <div class="belief-insight">
          <strong class="belief-insight__title">Key Insight</strong>
          <p class="belief-insight__text">Aschenbrenner's essay prompted divergent reactions. Andrey remained skeptical of the aggressive timeline and the implicit assumption that scaling necessarily produces AGI. Seth engaged more with the national security framing, accepting that if transformative AI emerges, governance questions become paramount. Both emphasized uncertainty: the scaling laws have empirical support but don't guarantee any particular capability threshold. The inability to predict breakthrough timing complicates both safety and policy planning.</p>
        </div>
        
      </div>
    </article>
    
  </div>

  <p class="beliefs-no-results" id="beliefs-no-results" hidden>
    No episodes match your search.
  </p>
</section>




      </div>
    </main>
    <footer class="site-footer">
  <div class="wrap">
    <div class="footer-row">
      <div class="footer-col">
        <div class="footer-title">Justified Posteriors</div>
        <div class="footer-text">A podcast exploring the economics of AI and innovation by deeply engaging with the literature.</div>
      </div>
      <div class="footer-col footer-links">
        <a href="https://empiricrafting.substack.com/" target="_blank" rel="noopener">Substack</a>
        <a href="https://podcasts.apple.com/us/podcast/justified-posteriors/id1784022930" target="_blank" rel="noopener">Apple Podcasts</a>
        <a href="https://open.spotify.com/show/6GAE7zHAnQyTaOprfEqJtQ" target="_blank" rel="noopener">Spotify</a>
        <a href="https://www.youtube.com/@justifiedposteriors" target="_blank" rel="noopener">YouTube</a>
      </div>
    </div>
    <div class="footer-meta">
      &copy; 2026 Andrey Fradkin & Seth Benzell
    </div>
  </div>
</footer>

<script src="/assets/js/beliefs.js"></script>

  </body>
</html>
