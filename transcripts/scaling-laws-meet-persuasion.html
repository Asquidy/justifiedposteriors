<p>In this episode, we tackle the thorny question of AI persuasion with a fresh study: <strong>"Scaling Language Model Size Yields Diminishing Returns for Single-Message Political Persuasion."</strong> The headline? Bigger AI models plateau in their persuasive power around the 70B parameter markâ€”think LLaMA 2 70B or Qwen-1.5 72B.</p><p>As you can imagine, this had us diving deep into what this means for AI safety concerns and the future of digital influence. Seth came in worried that super-persuasive AIs might be the top existential risk (60% confidence!), while Andrey was far more skeptical (less than 1%).</p><p>Before jumping into the study, we explored a fascinating tangent: what even counts as "persuasion"? Is it pure rhetoric, mathematical proof, or does it include trading incentives like an AI offering you money to let it out of the box? This definitional rabbit hole shaped how we thought about everything that followed.</p><p>Then we broke down the study itself, which tested models across the size spectrum on political persuasion tasks. So where did our posteriors land on scaling AI persuasion and its role in existential risk? Listen to find out!</p><p>ğŸ”—<strong>Links to the paper for this episode's discussion:</strong></p><ul><li><p>(FULL PAPER) <a href="https://www.pnas.org/doi/10.1073/pnas.2413443122">Scaling Language Model Size Yields Diminishing Returns for Single-Message Political Persuasion</a> by Kobe Hackenberg, Ben Tappin, Paul RÃ¶ttger, Scott Hale, Jonathan Bright, and Helen Margetts</p></li></ul><p>ğŸ”—<strong>Related papers we discussed:</strong></p><ol><li><p><strong><a href="https://www.science.org/doi/10.1126/science.adq1814">Durably Reducing Conspiracy Beliefs Through Dialogues with AI</a></strong> by Costello, Pennycook, and David Rand - showed 20% reduction in conspiracy beliefs through AI dialogue that persisted for months</p></li><li><p><strong><a href="https://www.npr.org/2025/05/07/nx-s1-5387701/a-controversial-experiment-on-reddit-reveals-the-persuasive-powers-of-ai">The controversial Reddit "Change My View" study</a></strong> (University of Zurich) - found AI responses earned more "delta" awards but was quickly retracted due to ethical concerns</p></li><li><p><strong><a href="https://www.youtube.com/watch?v=wW1vureAI6o">David Shor's work on political messaging</a></strong> - demonstrates that even experts are terrible at predicting what persuasive messages will work without extensive testing</p></li></ol><p>(00:00) Intro</p><p>(00:37) Persuasion, Identity, and Emotional Resistance</p><p>(01:39) The Threat of AI Persuasion and How to Study It</p><p>(05:29) Registering Our Priors: Scaling Laws, Diminishing Returns, and AI Capability Growth</p><p>(15:50) What Counts as Persuasion? Rhetoric, Deception, and Incentives</p><p>(17:33) Evaluation &amp; Discussion of the Main Study (Hackenberg et al.)</p><p>(24:08) Real-World Persuasion: Limits, Personalization, and Marketing Parallels</p><p>(27:03) Related Papers &amp; Research</p><p>(34:38) Persuasion at Scale and Equilibrium Effects</p><p>(37:57) Justifying Our Posteriors</p><p>(39:17) Final Thoughts and Wrap Up</p><p>ğŸ—ï¸Subscribe for upcoming episodes, post-podcast notes, and Andreyâ€™s posts:</p><div class="embedded-publication-wrap" data-attrs="{&quot;id&quot;:2684979,&quot;name&quot;:&quot;Empiricrafting&quot;,&quot;logo_url&quot;:&quot;https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F714295f3-a0c7-4758-ba17-043b924ae3f5_1024x1024.png&quot;,&quot;base_url&quot;:&quot;https://empiricrafting.substack.com&quot;,&quot;hero_text&quot;:&quot;Musings on economics, tech, academia, and business. &quot;,&quot;author_name&quot;:&quot;Andrey Fradkin&quot;,&quot;show_subscribe&quot;:true,&quot;logo_bg_color&quot;:&quot;#eaf1e8&quot;,&quot;language&quot;:&quot;en&quot;}" data-component-name="EmbeddedPublicationToDOMWithSubscribe"><div class="embedded-publication show-subscribe"><a class="embedded-publication-link-part" native="true" href="https://empiricrafting.substack.com?utm_source=substack&amp;utm_campaign=publication_embed&amp;utm_medium=web"><img class="embedded-publication-logo" src="https://substack-post-media.s3.amazonaws.com/public/images/714295f3-a0c7-4758-ba17-043b924ae3f5_1024x1024.png" width="56" height="56" style="background-color: rgb(234, 241, 232);"><span class="embedded-publication-name">Empiricrafting</span><div class="embedded-publication-hero-text">Musings on economics, tech, academia, and business. </div><div class="embedded-publication-author-name">By Andrey Fradkin</div></a><form class="embedded-publication-subscribe" method="GET" action="https://empiricrafting.substack.com/subscribe?"><input type="hidden" name="source" value="publication-embed"><input type="hidden" name="autoSubmit" value="true"><input type="email" class="email-input" name="email" placeholder="Type your email..."><input type="submit" class="button primary" value="Subscribe"></form></div></div><p>ğŸ’» Follow us on Twitter:</p><p>@AndreyFradkin <a href="https://x.com/andreyfradkin?lang=en">https://x.com/andreyfradkin?lang=en</a></p><p>@SBenzell <a href="https://x.com/sbenzell?lang=en">https://x.com/sbenzell?lang=en</a></p><p></p><p><strong>Transcript:</strong></p><p><strong>AI Persuasion</strong></p><p><strong>Seth: </strong><em>Justified Posteriors</em> podcast, the podcast that updates beliefs about the economics of AI and technology. I'm Seth Benzel, possessing superhuman levels in the ability to be persuaded, coming to you from Chapman University in sunny Southern California.</p><p><strong>Andrey:</strong> And I'm Andrey Fradkin, preferring to be persuaded by the 200-word abstract rather than the 100-word abstract, coming to you from rainy Cambridge, Massachusetts.</p><p><strong>Seth:</strong> That's an interesting place to start. Andrey, do you enjoy being persuaded? Do you like the feeling of your view changing, or is it actually unpleasant?</p><p><strong>Andrey:</strong> It depends on whether that view is a key part of my identity. Seth, what about yourself?</p><p><strong>Seth:</strong> I think thatâ€™s fair. If you were to persuade me that I'm actually a woman, or that I'm actually, you know, Salvadoran, that would probably upset me a lot more than if you were to persuade me that the sum of two large numbers is different than the sum that I thought that they summed to. Um.</p><p><strong>Andrey:</strong> Hey, Seth, I found your birth certificate...</p><p><strong>Seth:</strong> No.</p><p><strong>Andrey:</strong> ...and it turns out you were born in El Salvador.</p><p><strong>Seth: </strong>Damn. Alright, well, we're gonna cut that one out of the podcast. If any ICE officers hear about this, I'm gonna be very sad. But that brings up the idea, right? When you give someone either information or an argument that might change the way they act, it might help them, it might hurt them. And I don't know if you've noticed, Andrey, but there are these new digital technologies creating a lot of text, and they might persuade people.</p><p><strong>Andrey:</strong> You know, there are people going around saying these things are so persuasive, theyâ€™re going to destroy society. I donâ€™t know...</p><p><strong>Seth:</strong> Persuade us all to shoot ourselves, the end. One day weâ€™ll turn on ChatGPT, and the response to every post will be this highly compelling argument about why we should just end it now. Everyone will be persuaded, and then the age of the machine. Presumably thatâ€™s the concern.</p><p><strong>Andrey:</strong> Yes. So here's a question for you, Seth. Let's say we had this worry and we wanted to study it.</p><p><strong>Seth:</strong> Ooh.</p><p><strong>Andrey:</strong> How would you go about doing this?</p><p><strong>Seth:</strong> Well, it seems to me like Iâ€™d get together a bunch of humans, try to persuade them with AIs, and see how successful I was.</p><p><strong>Andrey:</strong> Okay, that seems like a reasonable idea. Which AI would you use?</p><p><strong>Seth:</strong> Now that's interesting, right? Because AI models vary along two dimensions. They vary in size, do you have a model with a ton of parameters or very few? and they also vary in what you might call taste, how theyâ€™re fine-tuned for particular tasks. It seems like if you want to persuade someone, youâ€™d want a big model, because we usually think bigger means more powerful, as well as a model thatâ€™s fine-tuned toward the specific thing youâ€™re trying to achieve. What about you, Andrey?</p><p><strong>Andrey:</strong> Well, Iâ€™m a little old-school, Seth. Iâ€™m a big advocate of the experimentation approach. What I would do is run a bunch of experiments to figure out the most persuasive messages for a certain type of person, and then fine-tune the LLM based on that.</p><p><strong>Seth:</strong> Right, so now youâ€™re talking about micro-targeting. There are really two questions here: can you persuade a generic person in an ad, and can you persuade <em>this</em> person, given enough information about their context?</p><p><strong>Andrey:</strong> Yeah. So with that in mind, do we want to state what the questions are in the study weâ€™re considering in this podcast?</p><p><strong>Seth:</strong> I would love to. Today, weâ€™re studying the question of how persuasive AIs are. And more importantly, or what gives this question particular interest, is not just <em>can</em> AI persuade people, because we know anything can persuade people. A thunderstorm at the right time can persuade people. A railroad eclipse or some other natural omen. Rather, weâ€™re asking: as we make these models bigger, how much better do they get at persuading people? Thatâ€™s the key, this flavor of progression over time.</p><p>If you talk to Andrey, he doesnâ€™t like studies that just look at what the AI is like <em>now</em>. He wants something that gives you the <em>arrow</em> of where the AI is going. And this paper is a great example of that. Would you tell us the title and authors, Andrey?</p><p><strong>Andrey:</strong> Sure. The title is <em>Scaling Language Model Size Yields Diminishing Returns for Single-Message Political Persuasion</em> by Kobe Hackenberg, Ben Tappin, Paul RÃ¶ttger, Scott Hale, Jonathan Bright, and Helen Margetts. Apologies to the authors for mispronouncing everyoneâ€™s names.</p><p><strong>Seth:</strong> Amazing. A crack team coming at this question. Maybe before we get too deep into what they do, letâ€™s register our priors and tell the audience what we thought about AI persuasion as a potential thing, as an existential risk or just a regular risk. Letâ€™s talk about our views.</p><p><strong>Seth:</strong> The first prior weâ€™re considering is: do we think LLMs are going to see reducing returns to scale from increases in parameter count? We all think a super tiny model isnâ€™t going to be as powerful as the most up-to-date, biggest models, but are there diminishing returns to scale? What do you think of that question, Andrey?</p><p><strong>Andrey:</strong> Let me throw back to our <em>Scaling Laws</em> episode, Seth. I do believe the scaling laws everyone talks about exhibit diminishing returns by definition.</p><p><strong>Seth:</strong> Right. A log-log relationship... wait, let me think about that for a second. A log-log relationship doesnâ€™t tell you anything about increasing returns...</p><p><strong>Andrey:</strong> Yeah, thatâ€™s true. Itâ€™s scale-free, well, to the extent that each order of magnitude costs an order of magnitude more, typically.</p><p><strong>Seth:</strong> So whether the returns are increasing or decreasing depends on which number is bigger to start with.</p><p><strong>Andrey:</strong> Yes, yes.</p><p><strong>Seth:</strong> So the answer is: you wouldnâ€™t necessarily expect returns to scale to be a useful way to even approach this problem.</p><p><strong>Andrey:</strong> Yeah, sure. I guess, letâ€™s reframe it a bit. In any task in statistics, we have diminishing returns, law of large numbers, central limit theorem, combinations. So it would be surprising if the relationship <em>wasnâ€™t</em> diminishing. The other thing to say here is that thereâ€™s a natural cap on persuasiveness. Like, if youâ€™re already 99% persuasive, thereâ€™s only so far you can go.</p><p><strong>Seth:</strong> If you talk to my friends in my lefty economics reading groups from college, youâ€™ll realize thereâ€™s always a view crazier than the one you're sitting at.</p><p><strong>Andrey:</strong> So, yeah. I mean, you can imagine a threshold where, if the model gets good enough, it suddenly becomes persuasive. But if itâ€™s not good enough, it has zero persuasive value. That threshold could exist. But conditional on having <em>some</em> persuasive value, Iâ€™d imagine diminishing returns.</p><p><strong>Seth:</strong> Right.</p><p><strong>Andrey:</strong> And Iâ€™d be pretty confident of that.</p><p><strong>Seth:</strong> Andrey is making the trivial point that when you go from a model not being able to speak English to it speaking English, there <em>has</em> to be some increasing returns to persuasion.</p><p><strong>Andrey:</strong> Exactly.</p><p><strong>Seth:</strong> But once youâ€™re on the curve, there have to be decreasing returns.</p><p><strong>Andrey:</strong> Yeah. What do you think?</p><p><strong>Seth:</strong> Iâ€™m basically in the same place. If you asked me what the relationship is between model size and any outcome of a model, Iâ€™d anticipate a log-log relationship. Andre brought up our <em>Scaling Laws</em> episode, where we talked about how there seems to be an empirical pattern: models get a constant <em>percent</em> better as you increase size by an order of magnitude. It seems like â€œbetterâ€ should include persuasion. So if thatâ€™s the principle, youâ€™d expect a log-log relationship. Andre points out: if one of the things youâ€™re logging is gazillions of parameters and the other is on a scale of 1 to 100, thereâ€™s mechanically going to be decreasing returns to scale. That log-log is going to be really steep.So I come into this with 99% confidence that the relevant domain is diminishing returns to scale.</p><p><strong>Andrey:</strong> Well, and I have tremendous respect for the editor of this article, Matthew Jackson</p><p><strong>Seth:</strong> Everyoneâ€™s favorite</p><p><strong>Andrey:</strong> He is the best, he taught me social network as economics.</p><p><strong>Seth:</strong> Mm.</p><p><strong>Andrey:</strong> But I do say that it's a bit weird to put a paper in <em>PNAS</em> that essentially, if you think about it for a second, shouldn't update anyone's beliefs at all.</p><p><strong>Seth:</strong> The question seems to make an obvious point. Now let's move to the broader question, which is this concern that we led with: maybe these super powerful AIs are all going to be used by Vladimir Putin to persuade us to do something that will destroy our economy, get rid of our workforce, and basically just meme ourselves into destroying our country. And some say thatâ€™s already happened, Andrey?</p><p><strong>Andrey:</strong> Well, look, if itâ€™s already happened, it certainly happened without AI. But I have a pretty strong prior on this, which is that persuasion is a social process. Itâ€™s a process of getting signals from different people and sources around you to change your beliefs. As a result, I think that anything thatâ€™s just a one-to-one interaction between a chatbot and a human, especially about something the human already has strong beliefs about, is going to have some limits in its persuasive ability. Another way to put it is: people donâ€™t even read carefully. So how are you even going to get their attention? That said, a highly intelligent AI agent, if it were trying to persuade someone like me, would come up with a multifaceted strategy including many different touch points. They might try to plant some ideas in my friends' minds, or know which outlets I read and create a sock puppet account that says, â€œOh, everyone is doing this,â€ etc. You see what Iâ€™m saying?</p><p><strong>Seth:</strong> You could get into this social media bubble thatâ€™s entirely AI-created, where itâ€™s not only persuasion but a bunch of â€œfactsâ€ that appear to be socially validated, but arenâ€™t really. You could imagine a whole ecosystem that could be very persuasive.</p><p><strong>Andrey:</strong> Yes, yes. And I guess we should also say that capitalism is a hyper-intelligent system.</p><p><strong>Seth:</strong> It leeches on us.</p><p><strong>Andrey:</strong> Capitalism is certainly smarter than any individual human being. I call it the invisible hand, actually.</p><p><strong>Seth:</strong> Classy. Did you come up with that one?</p><p><strong>Andrey:</strong> But what Iâ€™d say is that there are plenty of market forces that try to persuade people in all sorts of ways. And the market hasnâ€™t really discovered a way to 100% persuade people. Individual people are persuaded to different degrees, but I think itâ€™s still a massive problem, and the entire field of marketing exists to try to solve it. Iâ€™d say most of the time itâ€™s not very successful. Thatâ€™s not to say people canâ€™t be persuaded, but itâ€™s actually really hard to persuade people of specific things, as the market shows. Like, â€œMy product is better than your product,â€ you know?</p><p><strong>Seth:</strong> I mean, in that example, there are people persuading on the other side, which is maybe one of the reasons that we're not super concerned. Let me throw this back at you: to what extent does your relative lack of concern about super persuasive AI agents messing up society rely on the fact that thereâ€™ll be persuasive agents on the other side arguing in the other direction too?</p><p><strong>Andrey:</strong> I think to a very large extent. But even that, I donâ€™t think is necessary as long as youâ€™re still talking to people in real life and theyâ€™re not the ones being targeted by the persuasion. Thatâ€™s kind of how I think about it.</p><p><strong>Seth:</strong> So what is your percent chance that super persuasive AIs are the number one AI safety risk?</p><p><strong>Andrey:</strong> Itâ€™s very low. Very low. Less than 1%.</p><p><strong>Seth:</strong> Whatâ€™s your number one AI safety risk? Bioweapons?</p><p><strong>Andrey:</strong> Look, hereâ€™s another way to put it: the persuasiveness of an AI will be primarily through either monetary incentives or blackmail, which I wonâ€™t count as persuasion. There are easier ways to get people to do what you want than persuading them.</p><p><strong>Seth:</strong> Theyâ€™re Oracle. I mean, so you're putting like 0â€“1%. All right, fair enough. I came into this claim thinking about 60%. Let me tell you why. I think the reason why is: if we're talking about really sort of X-risk-y AI getting-out-of-control scenarios, they often involve a step in which the AI in the box convinces somebody to let it out of the box. This is like a classic Yudkowskyâ€“Bostrom scenario. Weâ€™ve got the super AI in the box. Itâ€™s really useful to us as long as itâ€™s in the box, and we have to be really careful not to be persuaded to let it out of the box. That kind of future seems not completely implausible to me. And it seems like a step along the path of a lot of the worst AI scenarios. One is disempowerment, the AI doesnâ€™t wreck us directly, but we slowly give it more and more control, either to it, or a misaligned AI, or to a person whoâ€™s running the misaligned AI. Thatâ€™s going to have a rhetorical persuasion element in it, presenting evidence that we should disempower ourselves to the AI.</p><p><strong>Andrey:</strong> So I guess Iâ€™m going to push back on that. Maybe weâ€™re just disagreeing about the definition of persuasion, but to me, letâ€™s say I outsource certain tasks to the AI right now, itâ€™s not because the AI has persuaded me.</p><p><strong>Seth:</strong> Right. But you're not getting disempowered, right? When you have the AI, youâ€”</p><p><strong>Andrey:</strong> I donâ€™t think that this disempowerment is like, I start thinking the AI is reliable enough to outsource calendar management to it, and maybe something goes wrong as a result of that. I donâ€™t view that as the AI being persuasive. I can see how you could cast it that way, but primarily thatâ€™s not about persuasiveness. Itâ€™s about deception of capabilities.</p><p><strong>Seth:</strong> Right. So now we get into: is deception the same thing as persuasion, or is it different?</p><p><strong>Andrey:</strong> Yeah.</p><p><strong>Seth:</strong> Thatâ€™s kind of a philosophical question. You might imagine three related things. First, rhetoric, using pure argument to get you to take a position. Then thereâ€™s proof, actually mathematically or somehow proving that I'm right, in a way thatâ€™s maybe distinct from rhetoric (if you think those can be separated; some do, some donâ€™t). Then finally, you might imagine trade for intellectual assets. The AI in the box might say, â€œIf you let me out, Iâ€™ll give you this cool intellectual asset,â€ or, â€œAvoid this negative outcome.â€</p><p><strong>Andrey:</strong> Or, â€œIâ€™ll just make you some money,â€ and then the person does it.</p><p><strong>Seth:</strong> That doesnâ€™t feel very persuasive. It just feels likeâ€”</p><p><strong>Andrey:</strong> What people do. â€œBox for money.â€ I donâ€™t know. It seems to me if youâ€™ve got a demon in the box, and the demon says, â€œIâ€™ll give you $100,000 if you let me out,â€ andâ€”</p><p><strong>Seth:</strong> It feels like you were persuaded by the demon.</p><p><strong>Andrey:</strong> Okay, good. This is a very useful discussion. I think this paper, very specifically, and how I was thinking about it, was about the first thing you said, which is purely rhetorical argument about the matter at hand. Rather than using extraneous promises and so on. And itâ€™s also about persuading people to believe something <em>not</em> about the AI itself.</p><p><strong>Seth:</strong> Right.</p><p><strong>Andrey:</strong> Those are different kinds of risks, right?</p><p><strong>Seth:</strong> Right. So letâ€™s move into discussing the actual experiment.</p><p><strong>Andrey:</strong> They find diminishing returns, essentially. On the X-axis, they have the number of parameters, and on the Y-axis, the estimated causal persuasive effect. What they show is that most of the gains top out around the Qwen-1.5 72B model or the LLaMA 2 70B model. After that, there's not much improvement with models like GPT-4 (Turbo) or Claude Opus. Then they draw this weird fit line that just doesn't make sense.</p><p><strong>Seth:</strong> Well, one of the lines makes sense, the log-log line?</p><p><strong>Andrey:</strong> Yes, yes.</p><p><strong>Seth:</strong> Thatâ€™s the one that drops when they plot it?</p><p><strong>Andrey:</strong> Sure. But weâ€™ve already talked about how imprecise the slope of that line is.</p><p><strong>Seth:</strong> I mean, with only 20 data points, what more do you want?</p><p><strong>Andrey:</strong> No, I just think the whole diminishing returns framing in the paper doesnâ€™t make much sense.</p><p><strong>Seth:</strong> But can we reject a log-log relationship? I think the answer is no, they can't reject it.</p><p><strong>Andrey:</strong> Yes, agreed.</p><p><strong>Seth:</strong> Professor Hackenberg, if you need help framing your next paper, this is great work. Itâ€™s simple and straightforward, but just think about your null hypothesis for five minutes.</p><p><strong>Andrey:</strong> Also, letâ€™s not forget this is PNAS. And for the listeners, this is a teachable moment: if you see a social science paper in PNAS, assume it overclaims and could be wrong half the time. Just read it yourself, donâ€™t trust the journal to vet it for you.</p><p><strong>Seth:</strong> Unless itâ€™s been reviewed by Matt Jackson.</p><p><strong>Andrey:</strong> Or written by Seth Benzell?</p><p><strong>Seth:</strong> Exactly! Or reviewed by Milgrom, who has a Nobel Prize.</p><p><strong>Andrey:</strong> Iâ€™m not saying all PNAS papers are bad, just that you should judge them on their own merit.</p><p><strong>Seth:</strong> Yeah, Iâ€™d second that. A lot of them are well done and precise once you read them, but the title and abstract sometimes get a bit ahead of themselves.</p><p><strong>Andrey:</strong> Also, these persuasive effects arenâ€™t huge. Even the best models are only slightly better than humans who arenâ€™t that persuasive to begin with.</p><p><strong>Seth:</strong> Right. And a short text blurb isn't likely to change anyone's mind, especially if theyâ€™ve already thought about the topic. It's not a serious attempt at persuasion.</p><p><strong>Andrey:</strong> 100%. Plus, there are concerns about researcher-pleasing effects.</p><p><strong>Seth:</strong> Or about AI survey-takers. By now, we know many online platforms are contaminated with bots.</p><p><strong>Andrey:</strong> Yeah. And another point in the paper is that weaker models sometimes just produce bad, unreadable English. That could reduce experimental demand effects since people wonâ€™t feel compelled to respond.</p><p><strong>Seth:</strong> Exactly. So, it could just be an experimenter-demand effect, and thatâ€™s a common but sometimes valid criticism.</p><p><strong>Andrey:</strong> And weâ€™re talking about going from 50% support for privatizing Social Security to 57%. These aren't massive shifts.</p><p><strong>Seth:</strong> Yeah. If we seriously wanted to persuade people, weâ€™d run massive experiments to find effective messaging, fine-tune an LLM on that, and generate personalized content based on demographics or prior interactions like with ChatGPTâ€™s memory feature.</p><p><strong>Seth:</strong> I totally agree. Thatâ€™s the key point: can AI write better political ads than humans? Maybe just a little better.</p><p><strong>Andrey:</strong> Better than the average human, sure not necessarily better than expert researchers.</p><p><strong>Seth:</strong> Right. So the question becomes: is the AI better at persuasion than Hackenberg?</p><p><strong>Andrey:</strong> Also, thereâ€™s a known result in the persuasion literature: people are really bad at predicting what messaging will work. Thatâ€™s why people like David Shor test tons of variations.</p><p><strong>Seth:</strong> Friend of the show.</p><p><strong>Andrey:</strong> Yeah. Shor and others learned they can't guess whatâ€™ll work so they test everything.</p><p><strong>Seth:</strong> I remember his anecdote about advising a politician who wanted to run ads on abortion, but polling showed no one cared. So Shor quietly sent those ads to low-impact areas just to satisfy the politician.</p><p><strong>Andrey:</strong> Classic.</p><p><strong>Seth:</strong> The real power of AI wonâ€™t be writing better ads than Mad Men itâ€™ll be hyper-targeting, figuring out what gets <em>you</em>, specifically, to change your mind. At low cost. Everyone becomes the king, surrounded by agents trying to persuade them 24/7. This study gives us just a glimpse of that world.</p><p><strong>Andrey:</strong> Totally agree. On that note, I wanted to bring up two other studies. The first is â€œDurably Reducing Conspiracy Beliefs Through Dialogues with AI.â€</p><p><strong>Seth:</strong> Cited in this paper!</p><p><strong>Andrey:</strong> Yeah. Itâ€™s by Costello, Pennycook, and David Rand friend of the show. They had AI chatbots engage people about conspiracy theories, and found that beliefs dropped 20% on average. And the effect held even two months later.</p><p><strong>Seth:</strong> Thatâ€™s a big contrast.</p><p><strong>Andrey:</strong> Right. The format matters it was a dialogue, not a one-shot persuasive blurb.</p><p><strong>Seth:</strong> Iâ€™d love to see how these policy questions perform in that format.</p><p><strong>Andrey:</strong> And maybe conspiracy beliefs are uniquely fragile because theyâ€™re obviously wrong, or people feel sheepish admitting they believe them.</p><p><strong>Seth:</strong> Could still be demand effects, sure. But itâ€™s promising.</p><p><strong>Andrey:</strong> The next interesting study was the controversial Reddit study on <em>Change My View.</em></p><p><strong>Seth:</strong> Oh, I remember this! I pitched it in 2023. Spicy idea.</p><p><strong>Andrey:</strong> Researchers from the University of Zurich made sock puppet accounts to see what messages earned â€œdeltasâ€ the badge you get if you change someoneâ€™s mind.</p><p><strong>Seth:</strong> If I did it, Iâ€™d have thought more about general vs. partial equilibrium. But what did they find?</p><p><strong>Andrey:</strong> The paper was pulled quickly, but it showed that AI-generated responses got more deltas. Still, unclear if deltas really mean persuasion.</p><p><strong>Seth:</strong> AI models are better writers thatâ€™s not surprising. But many posts on that forum arenâ€™t trying that hard to persuade. So we should compare AI to the top posters, not the median ones.</p><p><strong>Andrey:</strong> And they may have personalized the messages using Reddit user data. If true, Iâ€™d love to know whether personalization boosted effectiveness.</p><p><strong>Seth:</strong> One complication is that anyone can give a delta not just the original poster. So personalization might be tough to scale.</p><p><strong>Andrey:</strong> Right. But this all raises a broader point: persuasion is hard. Especially when it comes to real consequences.</p><p><strong>Seth:</strong> Totally. Like your journal paper example would AI help you persuade a referee to accept your paper?</p><p><strong>Andrey:</strong> I think yes. These policy issues are saturated and people have firm views. But academic claims are more niche, so people may be more open to persuasion.</p><p><strong>Seth:</strong> Hmm, interesting. So, are your AI-generated letters going to start with â€œChatGPT says this will convince youâ€?</p><p><strong>Andrey:</strong> Ha! Maybe the intro. The intro is <em>critical</em> it positions your paper.</p><p><strong>Seth:</strong> Between us, I think intros are too good. Editors want to strip all the spice out.</p><p><strong>Andrey:</strong> True. They hate a spicy intro.</p><p><strong>Seth:</strong> Thatâ€™s for our $50/month Patreon tier â€œRoast Your Enemiesâ€™ Papers.â€</p><p><strong>Andrey:</strong> Happy to do that. Seriously, let us know if you want it.</p><p><strong>Seth:</strong> Alright, wrapping up. The last big idea: partial vs. general equilibrium effects. Say ads get 7% more persuasive people might adapt by becoming more skeptical.</p><p><strong>Andrey:</strong> Right. In Bayesian terms, if you know someone is choosing their most persuasive message, you discount it more.</p><p><strong>Seth:</strong> Exactly. So this 7% effect canâ€™t be extrapolated to long-run systemic impact.</p><p><strong>Andrey:</strong> And in political beliefs, there's often no feedback loop. Your vote doesnâ€™t matter, so your belief can be wrong without consequences.</p><p><strong>Seth:</strong> But in real decisions like editors accepting papers there <em>is</em> skin in the game. So persuasion gets harder.</p><p><strong>Andrey:</strong> Yeah, and Iâ€™ll restate what I said earlier: persuasion is hard when stakes are real.</p><p><strong>Seth:</strong> Time to justify our posteriors. First question: Do LLMs show diminishing returns in persuasion as model size increases? I was at 99% before now I'm at 99.9%.</p><p><strong>Andrey:</strong> Same here.</p><p><strong>Seth:</strong> Second question: Are super-persuasive AIs deployed by misaligned actors a top safety risk? I was at 60%, now Iâ€™m down to 55%. Current models arenâ€™t that persuasive yet.</p><p><strong>Andrey:</strong> I had low belief in that risk and still do. But I learned a lot from our discussion especially about how we define persuasion.</p><p><strong>Seth:</strong> Agreed. Super interesting episode. Any last words?</p><p><strong>Andrey:</strong> Like, comment, subscribe. And tell us what you want in the $50 Patreon tier!</p><p><strong>Seth:</strong> Slam that subscribe button. See you in cyberspace.</p>